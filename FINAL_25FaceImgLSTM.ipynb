{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "FINAL_25FaceImgLSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvIwde2yGIk0"
      },
      "source": [
        "\"\"\" CODE FOR TRAINING PRETRAINED CNN + LSTM MODEL ON SEGMENTED FACE + ORIGINAL IMAGE (ENTIRE BODY) \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQzSi3bY6Pzh"
      },
      "source": [
        "pip install Pillow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1s5fybD6Qh6"
      },
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from imgaug import augmenters as iaa\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import time\n",
        "import os\n",
        "import glob\n",
        "import copy\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "from torchvision import transforms, utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn02P0_FV4c0"
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mI7T6nYhh3G"
      },
      "source": [
        "# mounting drive \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWzWEmC9sYEc"
      },
      "source": [
        "cd /content/drive/MyDrive/COS429_Final_Project/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JdU_Qq8QZJ0"
      },
      "source": [
        "class_name = {}\n",
        "label_names = []\n",
        "for i, label in enumerate(os.listdir('BOSTON_DATA_28_TRAINVAL/train/')):\n",
        "  label_names.append(label)\n",
        "  class_name[label] = i\n",
        "  print(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA9hktxNQid8"
      },
      "source": [
        "len(label_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AKuAM48ObRZ"
      },
      "source": [
        "cd BOSTON_DATA_28_FACES_TRAINVAL/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rktca0dqgJNx"
      },
      "source": [
        "# following this tutorial: https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\n",
        "# Top level data directory. Here we assume the format of the directory conforms\n",
        "#   to the ImageFolder structure\n",
        "# old image\n",
        "# path = '/content/drive/MyDrive/COS429_Final_Project/signs_images/train'\n",
        "\n",
        "# new path to get static images for CNN\n",
        "root_folder_name = 'BOSTON_DATA_28_TRAINVAL'\n",
        "path = '/content/drive/MyDrive/COS429_Final_Project/' + root_folder_name\n",
        "\n",
        "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
        "model_name = \"inception\"\n",
        "\n",
        "# Number of classes in the dataset\n",
        "num_classes = len(label_names) # change when done finetuning\n",
        "\n",
        "# Batch size for training (change depending on how much memory you have)\n",
        "batch_size = 16\n",
        "input_size = 224\n",
        "\n",
        "# Flag for feature extracting. When False, we finetune the whole model,\n",
        "#   when True we only update the reshaped layer params\n",
        "# feature_extract = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELZO-9odXzXa"
      },
      "source": [
        "def crop_center(img,cropx=112,cropy=224):\n",
        "    y,x,d = img.shape\n",
        "    startx = x//2-(cropx//2)\n",
        "    starty = y//2-(cropy//2)    \n",
        "    return img[starty:starty+cropy,startx:startx+cropx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kx2XKLZgULDw"
      },
      "source": [
        "# sampler is used to get valid indices for batches of sequences of imgs\n",
        "class ImageSampler(torch.utils.data.Sampler):\n",
        "    def __init__(self, end_idx, seq_length):        \n",
        "        indices = []\n",
        "        #print(end_idx)\n",
        "        for i in range(len(end_idx)-1):\n",
        "            start = end_idx[i]\n",
        "            end = end_idx[i+1] - seq_length\n",
        "            if (start > end): \n",
        "              break\n",
        "            indices.append(torch.arange(start, end))\n",
        "        indices = torch.cat(indices)\n",
        "        self.indices = indices\n",
        "        \n",
        "    def __iter__(self):\n",
        "        indices = self.indices[torch.randperm(len(self.indices))]\n",
        "        return iter(indices.tolist())\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "# homemade dataset to get sequences of images for LSTM \n",
        "class ImageSequenceDataset(Dataset):\n",
        "    def __init__(self, image_paths, seq_length, transform, length, train):\n",
        "        self.image_paths = image_paths\n",
        "        self.seq_length = seq_length\n",
        "        self.transform = transform\n",
        "        self.length = length\n",
        "        #AUG\n",
        "        self.train = train \n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        start = index\n",
        "        end = index + self.seq_length\n",
        "        #print('Getting images from {} to {}'.format(start, end))\n",
        "        indices = list(range(start, end))\n",
        "        images = []\n",
        "        #AUG\n",
        "        coarsedrop = np.random.uniform(0, 0.5)\n",
        "        sp = np.random.uniform()\n",
        "        gc = np.random.uniform(0.5, 2)\n",
        "        aug = iaa.Sequential([\n",
        "                         iaa.CoarseDropout(coarsedrop, size_percent=sp), \n",
        "                         iaa.GammaContrast(gc), \n",
        "                         iaa.Fliplr(p=1.0)])\n",
        "                         #iaa.AdditiveGaussianNoise(10,20)])\n",
        "        #AUG\n",
        "        p = np.random.uniform()\n",
        "        idx = 0 \n",
        "        while idx < len(indices):\n",
        "            i = indices[idx]\n",
        "            image_path = self.image_paths[i][0]\n",
        "            try:\n",
        "              image = cv2.imread(image_path)\n",
        "              image = cv2.resize(image, (input_size, input_size))\n",
        "              image = crop_center(image)\n",
        "              #print(image_path)\n",
        "              \n",
        "              #if not image: \n",
        "              #print('image shape done', image_path, image.shape)\n",
        "              test = image.shape\n",
        "              face_path_arr = image_path.split('/')\n",
        "              face_path_arr[5] = 'BOSTON_DATA_28_FACES_TRAINVAL'\n",
        "              face_path_arr[-1] = 'face-'+face_path_arr[-1]\n",
        "              face_path = \"/\".join(face_path_arr)\n",
        "              face = cv2.imread(face_path)\n",
        "              #print(face_path)\n",
        "              #if not face:\n",
        "              #print('face shape done', image_path, face.shape)\n",
        "              test = face.shape\n",
        "            except:\n",
        "              print('image shape bad', image_path, image.shape)\n",
        "              print('face shape bad', face_path, face.shape)\n",
        "              break\n",
        "            face = cv2.resize(face, (input_size // 2, input_size))\n",
        "            image = cv2.resize(image, (input_size // 2, input_size))\n",
        "            image = cv2.hconcat([image, face])\n",
        "            #AUG\n",
        "            if p > 0.5 and self.train:\n",
        "              image = aug(image=image)\n",
        "            image = Image.fromarray(image)\n",
        "            if self.transform:\n",
        "              image = self.transform(image)\n",
        "            images.append(image)\n",
        "            idx += 1\n",
        "        while len(images) != 0 and len(images) < 3: \n",
        "          images.append(images[-1])\n",
        "        image_stack = torch.stack(images)\n",
        "        print(self.image_paths[i][0].split('/')[7])\n",
        "        label = class_name[self.image_paths[i][0].split('/')[7]]\n",
        "        return image_stack, label\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0vhMvqJULF7"
      },
      "source": [
        "root_dir = '/content/drive/MyDrive/COS429_Final_Project/'+root_folder_name+'/train'\n",
        "class_paths = [d.path for d in os.scandir(root_dir) if d.is_dir]\n",
        "\n",
        "class_image_paths = []\n",
        "end_idx = []\n",
        "for c, class_path in enumerate(class_paths):\n",
        "    for d in os.scandir(class_path):\n",
        "        if d.is_dir:\n",
        "            paths = sorted(glob.glob(os.path.join(d.path, '*.jpg')))\n",
        "            #print(d.path)\n",
        "            # Add class idx to paths\n",
        "            paths = [(p, c) for p in paths]\n",
        "            class_image_paths.extend(paths)\n",
        "            end_idx.extend([len(paths)])\n",
        "\n",
        "end_idx = [0, *end_idx]\n",
        "end_idx = torch.cumsum(torch.tensor(end_idx), 0)\n",
        "seq_length = 3\n",
        "\n",
        "sampler = ImageSampler(end_idx, seq_length)\n",
        "input_size = 224\n",
        "\n",
        "# TODO: Fix transformer dimensions\n",
        "transform = transforms.Compose([\n",
        "        transforms.Resize((input_size)),  \n",
        "        transforms.CenterCrop(input_size),                     \n",
        "        # transforms.RandomResizedCrop(input_size),\n",
        "        # transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "dataset = ImageSequenceDataset(\n",
        "    image_paths=class_image_paths,\n",
        "    seq_length=seq_length,\n",
        "    transform=transform,\n",
        "    length=len(sampler), \n",
        "    train=True)\n",
        "\n",
        "\n",
        "loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=batch_size,\n",
        "    sampler=sampler\n",
        ")\n",
        "\n",
        "dataloaders_dict_train = {x: loader for x in ['train']}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLG6LNWe6WkU"
      },
      "source": [
        "# Display resulting image\n",
        "inputs, classes = next(iter(dataloaders_dict_train['train']))  \n",
        "\n",
        "clip = inputs[0]\n",
        "fig, axes = plt.subplots(2, seq_length)\n",
        "for img_idx in range(seq_length):\n",
        "    img = clip[img_idx]\n",
        "    img_display = torch.transpose(img.data, 0, 2)\n",
        "    img_display = torch.transpose(img_display, 0, 1)\n",
        "    axes[0, img_idx].imshow(img_display)\n",
        "    axes[0, img_idx].axis('off')\n",
        "    axes[0, 0].set_title('original clip')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCjgtpzNXQ9s"
      },
      "source": [
        "print('batches', classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXCjNSleULO2"
      },
      "source": [
        "root_dir = '/content/drive/MyDrive/COS429_Final_Project/'+root_folder_name+'/val'\n",
        "class_paths = [d.path for d in os.scandir(root_dir) if d.is_dir]\n",
        "\n",
        "class_image_paths = []\n",
        "end_idx = []\n",
        "for c, class_path in enumerate(class_paths):\n",
        "    for d in os.scandir(class_path):\n",
        "        if d.is_dir:\n",
        "            paths = sorted(glob.glob(os.path.join(d.path, '*.jpg')))\n",
        "            #print(d.path)\n",
        "            # Add class idx to paths\n",
        "            paths = [(p, c) for p in paths]\n",
        "            class_image_paths.extend(paths)\n",
        "            end_idx.extend([len(paths)])\n",
        "\n",
        "end_idx = [0, *end_idx]\n",
        "end_idx = torch.cumsum(torch.tensor(end_idx), 0)\n",
        "seq_length = 3\n",
        "\n",
        "sampler = ImageSampler(end_idx, seq_length)\n",
        "input_size = 224\n",
        "\n",
        "# TODO: Fix transformer dimensions\n",
        "transform = transforms.Compose([\n",
        "        transforms.Resize((input_size)),  \n",
        "        transforms.CenterCrop(input_size),                     \n",
        "        # transforms.RandomResizedCrop(input_size),\n",
        "        # transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "dataset = ImageSequenceDataset(\n",
        "    image_paths=class_image_paths,\n",
        "    seq_length=seq_length,\n",
        "    transform=transform,\n",
        "    length=len(sampler), \n",
        "    #AUG\n",
        "    train=False)\n",
        "\n",
        "\n",
        "\n",
        "loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=batch_size,\n",
        "    sampler=sampler\n",
        ")\n",
        "\n",
        "\n",
        "dataloaders_dict_val = {x: loader for x in ['val']}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66IjkoryaCvY"
      },
      "source": [
        "# Display resulting image from val\n",
        "inputs, classes = next(iter(dataloaders_dict_val['val']))  \n",
        "\n",
        "clip = inputs[0]\n",
        "fig, axes = plt.subplots(2, seq_length)\n",
        "for img_idx in range(seq_length):\n",
        "    img = clip[img_idx]\n",
        "    img_display = torch.transpose(img.data, 0, 2)\n",
        "    img_display = torch.transpose(img_display, 0, 1)\n",
        "    axes[0, img_idx].imshow(img_display)\n",
        "    axes[0, img_idx].axis('off')\n",
        "    axes[0, 0].set_title('original clip')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRQh5VkVqWf2"
      },
      "source": [
        "# Flag for feature extracting. When False, we finetune the whole model,\n",
        "#   when True we only update the reshaped layer params\n",
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "          # freezing\n",
        "            param.requires_grad = False\n",
        "# freezing\n",
        "#feature_extract = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhl3FRZjWZNj"
      },
      "source": [
        "class ResCRNN(nn.Module):\n",
        "    def __init__(self, sample_size=256, sample_duration=seq_length, num_classes=num_classes,\n",
        "                lstm_hidden_size=512, lstm_num_layers=1, arch=\"ours\",\n",
        "                attention=False):\n",
        "        super(ResCRNN, self).__init__()\n",
        "        self.sample_size = sample_size\n",
        "        self.sample_duration = sample_duration\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # network params\n",
        "        self.lstm_hidden_size = lstm_hidden_size\n",
        "        self.lstm_num_layers = lstm_num_layers\n",
        "        self.attention = attention\n",
        "\n",
        "        # network architecture\n",
        "        if arch == \"resnet18\":\n",
        "            resnet = models.resnet18(pretrained=True)\n",
        "        elif arch == \"resnet34\":\n",
        "            resnet = models.resnet34(pretrained=True)\n",
        "        elif arch == \"resnet50\":\n",
        "            resnet = models.resnet50(pretrained=True)\n",
        "        elif arch == \"resnet101\":\n",
        "            resnet = models.resnet101(pretrained=True)\n",
        "        elif arch == \"resnet152\":\n",
        "            resnet = models.resnet152(pretrained=True)\n",
        "        elif arch == \"ours\":\n",
        "          resnet = torch.load('/content/drive/MyDrive/COS429_Final_Project/signs_img_cnn/signs_images/model.npz')\n",
        "          set_parameter_requires_grad(resnet, feature_extracting=True)\n",
        "        # delete the last fc layer\n",
        "        modules = list(resnet.children())[:-1]\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "        in_features = list(resnet.children())[-1][1].in_features\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=in_features,\n",
        "            hidden_size=self.lstm_hidden_size,\n",
        "            num_layers=self.lstm_num_layers,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        if self.attention:\n",
        "            self.attn_block = LSTMAttentionBlock(hidden_size=self.lstm_hidden_size)\n",
        "        self.fc1 = nn.Linear(self.lstm_hidden_size, self.num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # CNN\n",
        "        cnn_embed_seq = []\n",
        "        # x: (batch_size, t, channel, h, w)\n",
        "        for t in range(x.size(1)):\n",
        "            # with torch.no_grad():\n",
        "            out = self.resnet(x[:, t, :, :, :])\n",
        "            # print(out.shape)\n",
        "            out = out.view(out.size(0), -1)\n",
        "            cnn_embed_seq.append(out)\n",
        "\n",
        "        cnn_embed_seq = torch.stack(cnn_embed_seq, dim=0)\n",
        "        # print(cnn_embed_seq.shape)\n",
        "        # batch first\n",
        "        cnn_embed_seq = cnn_embed_seq.transpose_(0, 1)\n",
        "\n",
        "        # LSTM\n",
        "        # use faster code paths\n",
        "        self.lstm.flatten_parameters()\n",
        "        out, (h_n, c_n) = self.lstm(cnn_embed_seq, None)\n",
        "        # MLP\n",
        "        if self.attention:\n",
        "            out = self.fc1(self.attn_block(out))\n",
        "        else:\n",
        "            # out: (batch, seq, feature), choose the last time step\n",
        "            out = self.fc1(out[:, -1, :])\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeMlk5lmWZP4"
      },
      "source": [
        "import torch\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def train_epoch(model, criterion, optimizer, dataloader, device, epoch, logger, log_interval, writer):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    all_label = []\n",
        "    all_pred = []\n",
        "\n",
        "    for batch_idx, data in enumerate(dataloader):\n",
        "        # get the inputs and labels\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # forward\n",
        "        outputs = model(inputs)\n",
        "        if isinstance(outputs, list):\n",
        "            outputs = outputs[0]\n",
        "\n",
        "        # compute the loss\n",
        "        #print('labels into loss function: ', labels)\n",
        "        loss = criterion(outputs, labels.squeeze())\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # compute the accuracy\n",
        "        prediction = torch.max(outputs, 1)[1]\n",
        "        all_label.extend(labels.squeeze())\n",
        "        all_pred.extend(prediction)\n",
        "        score = accuracy_score(labels.squeeze().cpu().data.squeeze().numpy(), prediction.cpu().data.squeeze().numpy())\n",
        "\n",
        "        # backward & optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (batch_idx + 1) % log_interval == 0:\n",
        "            print(\"epoch {:3d} | iteration {:5d} | Loss {:.6f} | Acc {:.2f}%\".format(epoch+1, batch_idx+1, loss.item(), score*100))\n",
        "            logger.info(\"epoch {:3d} | iteration {:5d} | Loss {:.6f} | Acc {:.2f}%\".format(epoch+1, batch_idx+1, loss.item(), score*100))\n",
        "\n",
        "    # Compute the average loss & accuracy\n",
        "    training_loss = sum(losses)/len(losses)\n",
        "    all_label = torch.stack(all_label, dim=0)\n",
        "    all_pred = torch.stack(all_pred, dim=0)\n",
        "    training_acc = accuracy_score(all_label.squeeze().cpu().data.squeeze().numpy(), all_pred.cpu().data.squeeze().numpy())\n",
        "    # Log\n",
        "    writer.add_scalars('Loss', {'train': training_loss}, epoch+1)\n",
        "    writer.add_scalars('Accuracy', {'train': training_acc}, epoch+1)\n",
        "    logger.info(\"Average Training Loss of Epoch {}: {:.6f} | Acc: {:.2f}%\".format(epoch+1, training_loss, training_acc*100))\n",
        "    print(\"Average Training Loss of Epoch {}: {:.6f} | Acc: {:.2f}%\".format(epoch+1, training_loss, training_acc*100))\n",
        "    return training_loss, training_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0qioIegWZSj"
      },
      "source": [
        "def val_epoch(model, criterion, dataloader, device, epoch, logger, writer, class_acc, class_occ):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    all_label = []\n",
        "    all_pred = []\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, data in enumerate(dataloader):\n",
        "            # get the inputs and labels\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "            # forward\n",
        "            outputs = model(inputs)\n",
        "            if isinstance(outputs, list):\n",
        "                outputs = outputs[0]\n",
        "            # compute the loss\n",
        "            #print('labels into loss function: ', labels)\n",
        "            #print(outputs.size(0))\n",
        "            if outputs.size(0) < 3: \n",
        "              print('break')\n",
        "              break\n",
        "            #print(labels.squeeze().size(0))\n",
        "            loss = criterion(outputs, labels.squeeze())\n",
        "            losses.append(loss.item())\n",
        "            # collect labels & prediction\n",
        "            prediction = torch.max(outputs, 1)[1]\n",
        "            all_label.extend(labels.squeeze())\n",
        "            all_pred.extend(prediction)\n",
        "            np_labels = labels.squeeze().cpu().detach().numpy() \n",
        "            #print(np_labels)\n",
        "            np_preds = prediction.cpu().detach().numpy() \n",
        "            #print(np_preds)\n",
        "            for i in range(len(np_labels)):\n",
        "              class_occ[np_labels[i]]+=1\n",
        "              if(np_labels[i] == np_preds[i]):\n",
        "                class_acc[np_preds[i]]+=1\n",
        "    # Compute the average loss & accuracy\n",
        "    validation_loss = sum(losses)/len(losses)\n",
        "    all_label = torch.stack(all_label, dim=0)\n",
        "    all_pred = torch.stack(all_pred, dim=0)\n",
        "    validation_acc = accuracy_score(all_label.squeeze().cpu().data.squeeze().numpy(), all_pred.cpu().data.squeeze().numpy())\n",
        "    # Log\n",
        "    writer.add_scalars('Loss', {'validation': validation_loss}, epoch+1)\n",
        "    writer.add_scalars('Accuracy', {'validation': validation_acc}, epoch+1)\n",
        "    print(\"Average Validation Loss of Epoch {}: {:.6f} | Acc: {:.2f}%\".format(epoch+1, validation_loss, validation_acc*100))\n",
        "    logger.info(\"Average Validation Loss of Epoch {}: {:.6f} | Acc: {:.2f}%\".format(epoch+1, validation_loss, validation_acc*100))\n",
        "    return validation_loss, validation_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSSwCEVbHhSf"
      },
      "source": [
        "def plot_class(class_acc, title, ylab): \n",
        "  fig = plt.figure()\n",
        "  ax = fig.add_axes([0,0,1,1])\n",
        "  signs = ['ALWAYS', 'BEAUTIFUL', 'BIG', 'BORED', 'BORN', 'BREAK', 'BREAKDOWN', 'CANCEL', 'CANNOT', \n",
        "      'CHAT', 'CLEAN', 'CORRECT', 'CRASH', 'FAVORITE', 'FRIEND', 'HAPPY', 'LIVE', 'LOVE', 'NO', \n",
        "      'NONE', 'PARTY', 'SICK', 'THANK YOU', 'WHAT', 'WHY']\n",
        "  plt.xticks(rotation='vertical')\n",
        "  ax.bar(signs, class_acc)\n",
        "  plt.title(title)\n",
        "  plt.xlabel(\"Signs\")\n",
        "  plt.ylabel(ylab)\n",
        "  plt.show()\n",
        "  #plt.savefig('/content/drive/MyDrive/COS429_Final_Project/'+ root_folder_name + '/plot/faceimg'+ ylab + '.png')\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqmbrZ50pdCJ"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "from datetime import datetime\n",
        "import logging\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fdqhLUJ1WZUs",
        "outputId": "bf8ea717-8b51-4714-ae10-2f5788c50509"
      },
      "source": [
        "\n",
        "'''\n",
        "from models.ConvLSTM import CRNN, ResCRNN\n",
        "from dataset import CSL_Isolated\n",
        "from train import train_epoch\n",
        "from validation import val_epoch\n",
        "'''\n",
        "\n",
        "# Path setting\n",
        "model_path = '/content/drive/MyDrive/COS429_Final_Project/'+root_folder_name+'/cnnlstm_models'\n",
        "log_path = '/content/drive/MyDrive/COS429_Final_Project/'+root_folder_name+'/cnnlstm_{:%Y-%m-%d_%H-%M-%S}.log'.format(datetime.now())\n",
        "sum_path = '/content/drive/MyDrive/COS429_Final_Project/'+root_folder_name+'/runs/slr_cnnlstm_{:%Y-%m-%d_%H-%M-%S}'.format(datetime.now())\n",
        "\n",
        "# Log to file & tensorboard writer\n",
        "for handler in logging.root.handlers[:]:\n",
        "    logging.root.removeHandler(handler)\n",
        "logging.basicConfig(level=logging.INFO, format='%(message)s', handlers=[logging.FileHandler(log_path), logging.StreamHandler()])\n",
        "logger = logging.getLogger('SLR')\n",
        "logger.info('Logging to file...')\n",
        "logger.error('This should go to both console and file')\n",
        "writer = SummaryWriter(sum_path)\n",
        "\n",
        "# Device setting\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparams\n",
        "epochs = 15\n",
        "batch_size = batch_size #3\n",
        "learning_rate = 7.618935788137979e-05\n",
        "weight_decay = 0.02587591092706272\n",
        "log_interval = 20\n",
        "sample_size = input_size #224\n",
        "sample_duration = seq_length # 3\n",
        "num_classes = num_classes # 25\n",
        "lstm_hidden_size = 512\n",
        "lstm_num_layers = 1\n",
        "attention = False\n",
        "model_count = 'face-img'\n",
        "class_acc = np.zeros(25)\n",
        "class_occ = np.zeros(25)\n",
        "# Train with Conv+LSTM\n",
        "if __name__ == '__main__':\n",
        "    # Load data\n",
        "    train_loader = dataloaders_dict_train['train']\n",
        "    val_loader = dataloaders_dict_val['val']\n",
        "    # Create model\n",
        "    #model = CRNN(sample_size=sample_size, sample_duration=sample_duration, num_classes=num_classes,\n",
        "    #             lstm_hidden_size=lstm_hidden_size, lstm_num_layers=lstm_num_layers).to(device)\n",
        "    model = ResCRNN(sample_size=sample_size, sample_duration=sample_duration, num_classes=num_classes,\n",
        "                lstm_hidden_size=lstm_hidden_size, lstm_num_layers=lstm_num_layers, attention=attention).to(device)\n",
        "    # Run the model parallelly\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        logger.info(\"Using {} GPUs\".format(torch.cuda.device_count()))\n",
        "        model = nn.DataParallel(model)\n",
        "    # Create loss criterion & optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    # Start training\n",
        "    logger.info(\"Training Started\".center(60, '#'))\n",
        "    train_loss = []\n",
        "    train_acc = []\n",
        "    val_loss = []\n",
        "    val_acc = []\n",
        "    for epoch in range(epochs):\n",
        "        print('Epoch: ', epoch)\n",
        "        # Train the model\n",
        "        print('training model')\n",
        "        training_loss, training_acc = train_epoch(model, criterion, optimizer, train_loader, device, epoch, logger, log_interval, writer)\n",
        "        train_loss.append(training_loss)\n",
        "        train_acc.append(training_acc)\n",
        "\n",
        "        # Validate the model\n",
        "        print('validating model')\n",
        "        valid_loss, valid_acc =val_epoch(model, criterion, val_loader, device, epoch, logger, writer, class_acc, class_occ)\n",
        "        val_loss.append(valid_loss)\n",
        "        val_acc.append(valid_acc)\n",
        "\n",
        "        # Save model\n",
        "        torch.save(model.state_dict(), os.path.join(model_path, \"face-img_epoch{:03d}.pth\".format(epoch+1)))\n",
        "        logger.info(\"Epoch {} Model Saved\".format(epoch+1).center(60, '#'))\n",
        "        writer.add_scalar('Loss/train', training_loss, epoch)\n",
        "        writer.add_scalar('Loss/test', valid_loss, epoch)\n",
        "        writer.add_scalar('Accuracy/train', valid_loss, epoch)\n",
        "        writer.add_scalar('Accuracy/test', valid_acc, epoch)\n",
        "\n",
        "    logger.info(\"Training Finished\".center(60, '#'))\n",
        "\n",
        "    plt.plot(train_loss, '-o', color='red', label='train-loss')\n",
        "    plt.plot(train_acc, '-o', color='green', label='train-acc')\n",
        "    plt.plot(val_loss, '-o', color='blue', label='val-loss')\n",
        "    plt.plot(val_acc, '-o', color='orange', label='val-acc')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.savefig('/content/drive/MyDrive/COS429_Final_Project/'+ root_folder_name + '/plot/model' + str(model_count) + '.png')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logging to file...\n",
            "This should go to both console and file\n",
            "######################Training Started######################\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  0\n",
            "training model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   1 | iteration    20 | Loss 1.868043 | Acc 68.75%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   1 | iteration    20 | Loss 1.868043 | Acc 68.75%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   1 | iteration    40 | Loss 1.093596 | Acc 75.00%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   1 | iteration    40 | Loss 1.093596 | Acc 75.00%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Training Loss of Epoch 1: 2.033668 | Acc: 49.10%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Training Loss of Epoch 1: 2.033668 | Acc: 49.10%\n",
            "validating model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Loss of Epoch 1: 3.393679 | Acc: 14.25%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Loss of Epoch 1: 3.393679 | Acc: 14.25%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "####################Epoch 1 Model Saved#####################\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  1\n",
            "training model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   2 | iteration    20 | Loss 1.112431 | Acc 62.50%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   2 | iteration    20 | Loss 1.112431 | Acc 62.50%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   2 | iteration    40 | Loss 1.064473 | Acc 75.00%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   2 | iteration    40 | Loss 1.064473 | Acc 75.00%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Training Loss of Epoch 2: 1.226547 | Acc: 65.15%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Training Loss of Epoch 2: 1.226547 | Acc: 65.15%\n",
            "validating model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Loss of Epoch 2: 3.641284 | Acc: 31.73%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Loss of Epoch 2: 3.641284 | Acc: 31.73%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "####################Epoch 2 Model Saved#####################\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  2\n",
            "training model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   3 | iteration    20 | Loss 1.406753 | Acc 56.25%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   3 | iteration    20 | Loss 1.406753 | Acc 56.25%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   3 | iteration    40 | Loss 0.923169 | Acc 68.75%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   3 | iteration    40 | Loss 0.923169 | Acc 68.75%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Training Loss of Epoch 3: 1.035687 | Acc: 66.80%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Training Loss of Epoch 3: 1.035687 | Acc: 66.80%\n",
            "validating model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Loss of Epoch 3: 3.819449 | Acc: 36.26%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Loss of Epoch 3: 3.819449 | Acc: 36.26%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "####################Epoch 3 Model Saved#####################\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  3\n",
            "training model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   4 | iteration    20 | Loss 1.010841 | Acc 62.50%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   4 | iteration    20 | Loss 1.010841 | Acc 62.50%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   4 | iteration    40 | Loss 1.056521 | Acc 50.00%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   4 | iteration    40 | Loss 1.056521 | Acc 50.00%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Training Loss of Epoch 4: 0.925023 | Acc: 69.57%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Training Loss of Epoch 4: 0.925023 | Acc: 69.57%\n",
            "validating model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Loss of Epoch 4: 3.869302 | Acc: 33.85%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Loss of Epoch 4: 3.869302 | Acc: 33.85%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "####################Epoch 4 Model Saved#####################\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  4\n",
            "training model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   5 | iteration    20 | Loss 0.491556 | Acc 93.75%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   5 | iteration    20 | Loss 0.491556 | Acc 93.75%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   5 | iteration    40 | Loss 0.538636 | Acc 87.50%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   5 | iteration    40 | Loss 0.538636 | Acc 87.50%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Training Loss of Epoch 5: 0.875339 | Acc: 71.37%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Training Loss of Epoch 5: 0.875339 | Acc: 71.37%\n",
            "validating model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Loss of Epoch 5: 4.010530 | Acc: 32.02%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Loss of Epoch 5: 4.010530 | Acc: 32.02%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "####################Epoch 5 Model Saved#####################\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  5\n",
            "training model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   6 | iteration    20 | Loss 1.086713 | Acc 50.00%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   6 | iteration    20 | Loss 1.086713 | Acc 50.00%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   6 | iteration    40 | Loss 0.837553 | Acc 68.75%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   6 | iteration    40 | Loss 0.837553 | Acc 68.75%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Training Loss of Epoch 6: 0.828532 | Acc: 70.68%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Training Loss of Epoch 6: 0.828532 | Acc: 70.68%\n",
            "validating model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Loss of Epoch 6: 4.133858 | Acc: 22.73%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Loss of Epoch 6: 4.133858 | Acc: 22.73%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "####################Epoch 6 Model Saved#####################\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  6\n",
            "training model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   7 | iteration    20 | Loss 0.628375 | Acc 68.75%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   7 | iteration    20 | Loss 0.628375 | Acc 68.75%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   7 | iteration    40 | Loss 0.977988 | Acc 75.00%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   7 | iteration    40 | Loss 0.977988 | Acc 75.00%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Training Loss of Epoch 7: 0.829182 | Acc: 72.48%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Training Loss of Epoch 7: 0.829182 | Acc: 72.48%\n",
            "validating model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Loss of Epoch 7: 4.076346 | Acc: 30.99%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Loss of Epoch 7: 4.076346 | Acc: 30.99%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "####################Epoch 7 Model Saved#####################\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  7\n",
            "training model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   8 | iteration    20 | Loss 0.643644 | Acc 75.00%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   8 | iteration    20 | Loss 0.643644 | Acc 75.00%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   8 | iteration    40 | Loss 0.642249 | Acc 75.00%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   8 | iteration    40 | Loss 0.642249 | Acc 75.00%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Training Loss of Epoch 8: 0.782942 | Acc: 73.72%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Training Loss of Epoch 8: 0.782942 | Acc: 73.72%\n",
            "validating model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Loss of Epoch 8: 4.286648 | Acc: 27.19%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Loss of Epoch 8: 4.286648 | Acc: 27.19%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "####################Epoch 8 Model Saved#####################\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  8\n",
            "training model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   9 | iteration    20 | Loss 0.389788 | Acc 93.75%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   9 | iteration    20 | Loss 0.389788 | Acc 93.75%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   9 | iteration    40 | Loss 0.751330 | Acc 75.00%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   9 | iteration    40 | Loss 0.751330 | Acc 75.00%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Training Loss of Epoch 9: 0.743482 | Acc: 74.69%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Training Loss of Epoch 9: 0.743482 | Acc: 74.69%\n",
            "validating model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Loss of Epoch 9: 4.259354 | Acc: 33.70%\n",
            "####################Epoch 9 Model Saved#####################\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Loss of Epoch 9: 4.259354 | Acc: 33.70%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  10 | iteration    20 | Loss 0.954679 | Acc 62.50%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  9\n",
            "training model\n",
            "epoch  10 | iteration    20 | Loss 0.954679 | Acc 62.50%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  10 | iteration    40 | Loss 1.007621 | Acc 68.75%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  10 | iteration    40 | Loss 1.007621 | Acc 68.75%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Training Loss of Epoch 10: 0.723773 | Acc: 74.14%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Training Loss of Epoch 10: 0.723773 | Acc: 74.14%\n",
            "validating model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Loss of Epoch 10: 4.335642 | Acc: 33.33%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Loss of Epoch 10: 4.335642 | Acc: 33.33%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "####################Epoch 10 Model Saved####################\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  10\n",
            "training model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  11 | iteration    20 | Loss 0.702490 | Acc 81.25%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  11 | iteration    20 | Loss 0.702490 | Acc 81.25%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  11 | iteration    40 | Loss 0.614309 | Acc 75.00%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  11 | iteration    40 | Loss 0.614309 | Acc 75.00%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Training Loss of Epoch 11: 0.700429 | Acc: 75.52%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Training Loss of Epoch 11: 0.700429 | Acc: 75.52%\n",
            "validating model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Loss of Epoch 11: 4.289415 | Acc: 33.19%\n",
            "####################Epoch 11 Model Saved####################\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Loss of Epoch 11: 4.289415 | Acc: 33.19%\n",
            "Epoch:  11\n",
            "training model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  12 | iteration    20 | Loss 0.597949 | Acc 75.00%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  12 | iteration    20 | Loss 0.597949 | Acc 75.00%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  12 | iteration    40 | Loss 0.886904 | Acc 68.75%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  12 | iteration    40 | Loss 0.886904 | Acc 68.75%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Training Loss of Epoch 12: 0.666613 | Acc: 75.93%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Training Loss of Epoch 12: 0.666613 | Acc: 75.93%\n",
            "validating model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Loss of Epoch 12: 4.341520 | Acc: 32.68%\n",
            "####################Epoch 12 Model Saved####################\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Loss of Epoch 12: 4.341520 | Acc: 32.68%\n",
            "Epoch:  12\n",
            "training model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  13 | iteration    20 | Loss 0.643706 | Acc 87.50%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  13 | iteration    20 | Loss 0.643706 | Acc 87.50%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  13 | iteration    40 | Loss 1.268787 | Acc 56.25%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  13 | iteration    40 | Loss 1.268787 | Acc 56.25%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Training Loss of Epoch 13: 0.672144 | Acc: 78.01%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Training Loss of Epoch 13: 0.672144 | Acc: 78.01%\n",
            "validating model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Loss of Epoch 13: 4.167084 | Acc: 29.82%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Loss of Epoch 13: 4.167084 | Acc: 29.82%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "####################Epoch 13 Model Saved####################\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  13\n",
            "training model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  14 | iteration    20 | Loss 0.408776 | Acc 87.50%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  14 | iteration    20 | Loss 0.408776 | Acc 87.50%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  14 | iteration    40 | Loss 0.872597 | Acc 81.25%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  14 | iteration    40 | Loss 0.872597 | Acc 81.25%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Training Loss of Epoch 14: 0.650563 | Acc: 78.98%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Training Loss of Epoch 14: 0.650563 | Acc: 78.98%\n",
            "validating model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Loss of Epoch 14: 4.464516 | Acc: 30.77%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Loss of Epoch 14: 4.464516 | Acc: 30.77%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "####################Epoch 14 Model Saved####################\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  14\n",
            "training model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  15 | iteration    20 | Loss 0.338154 | Acc 93.75%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  15 | iteration    20 | Loss 0.338154 | Acc 93.75%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  15 | iteration    40 | Loss 0.575484 | Acc 81.25%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  15 | iteration    40 | Loss 0.575484 | Acc 81.25%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Training Loss of Epoch 15: 0.716417 | Acc: 75.52%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Training Loss of Epoch 15: 0.716417 | Acc: 75.52%\n",
            "validating model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Loss of Epoch 15: 4.873075 | Acc: 17.25%\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Loss of Epoch 15: 4.873075 | Acc: 17.25%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "####################Epoch 15 Model Saved####################\n",
            "#####################Training Finished######################\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cs09wT9ZP0ny"
      },
      "source": [
        "print('class accuracies', class_acc)\n",
        "print('class occurrences', class_occ)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6UlMRAYDqOM"
      },
      "source": [
        "plot_class(class_acc, 'Validation Accuracy Rates Per Sign', \"Accuracy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_7q1cYXQEbF"
      },
      "source": [
        "plot_class(class_occ, 'Occurrence of Sign', \"Occurence\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzYOfSDnzGDQ"
      },
      "source": [
        "max_count = 30\n",
        "def hyperparamsearch():\n",
        "  for handler in logging.root.handlers[:]:\n",
        "    logging.root.removeHandler(handler)\n",
        "  logh_path = \"/content/drive/MyDrive/COS429_Final_Project/BOSTON_DATA_28_TRAINVAL/logh/params_{:%Y-%m-%d_%H-%M-%S}.log\".format(datetime.now())\n",
        "  sumh_path = \"/content/drive/MyDrive/COS429_Final_Project/BOSTON_DATA_28_TRAINVAL/runs/slr_cnnlstm_{:%Y-%m-%d_%H-%M-%S}\".format(datetime.now())\n",
        "  # Log to file & tensorboard writer\n",
        "  logging.basicConfig(level=logging.INFO, format='%(message)s', handlers=[logging.FileHandler(logh_path), logging.StreamHandler()])\n",
        "  logger = logging.getLogger('SLR')\n",
        "  logger.info('Logging to file...')\n",
        "  logger.error('This should go to both console and file')\n",
        "  writer = SummaryWriter(sumh_path)\n",
        "  epochs = 3\n",
        "  logger.info(\"Hyper Params Started\".center(60, '#'))\n",
        "  for count in range(max_count):\n",
        "    wd = 10**np.random.uniform(-2, -1)\n",
        "    lr = 10**np.random.uniform(-4, -5)\n",
        "    logger.info(\"lr {} wd {}\".format(lr, wd))\n",
        "    # init model \n",
        "    model = ResCRNN(sample_size=sample_size, sample_duration=sample_duration, num_classes=num_classes,\n",
        "              lstm_hidden_size=lstm_hidden_size, lstm_num_layers=lstm_num_layers, attention=attention).to(device)\n",
        "    # Run the model parallelly\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        logger.info(\"Using {} GPUs\".format(torch.cuda.device_count()))\n",
        "        model = nn.DataParallel(model)\n",
        "    # Create loss criterion & optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
        "\n",
        "    # Start training\n",
        "    train_loss = []\n",
        "    train_acc = []\n",
        "    val_loss = []\n",
        "    val_acc = []\n",
        "    for epoch in range(epochs):\n",
        "        print('Epoch: ', epoch)\n",
        "        # Train the model\n",
        "        print('training model')\n",
        "        training_loss, training_acc = train_epoch(model, criterion, optimizer, train_loader, device, epoch, logger, log_interval, writer)\n",
        "        train_loss.append(training_loss)\n",
        "        train_acc.append(training_acc)\n",
        "        \n",
        "        # Validate the model\n",
        "        print('validating model')\n",
        "        valid_loss, valid_acc =val_epoch(model, criterion, val_loader, device, epoch, logger, writer)\n",
        "        val_loss.append(valid_loss)\n",
        "        val_acc.append(valid_acc)\n",
        "\n",
        "        # Save model\n",
        "        #torch.save(model.state_dict(), os.path.join(model_path, \"slr_hyperparam_epoch{:03d}.pth\".format(epoch+1)))\n",
        "        logger.info(\"Epoch {} Model Saved\".format(epoch+1).center(60, '#'))\n",
        "    plt.clf()\n",
        "    plt.title('Hyperparams Accuracy and Loss')\n",
        "    plt.plot(train_loss, '-o', color='red')\n",
        "    plt.plot(train_acc, '-o', color='green')\n",
        "    plt.plot(val_loss, '-o', color='blue')\n",
        "    plt.plot(val_acc, '-o', color='orange')\n",
        "    plt.savefig('/content/drive/MyDrive/COS429_Final_Project/BOSTON_DATA_28_TRAINVAL/plot/' + 'hyperparam-lr' + str(lr) + 'wd' + str(wd) + '.png')\n",
        "\n",
        "  logger.info(\"HyperParams Finished\".center(60, '#'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMe5qfarzA-K"
      },
      "source": [
        "hyperparamsearch()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}